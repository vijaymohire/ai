{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training with TFX and Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1.  Use the TFX CLI to build a TFX pipeline.\n",
    "2.  Deploy a TFX pipeline version without tuning to a hosted AI Platform Pipelines instance.\n",
    "3.  Create and monitor a TFX pipeline run using the TFX CLI.\n",
    "4.  Deploy a new TFX pipeline version with tuning enabled to a hosted AI Platform Pipelines instance.\n",
    "5.  Create and monitor another TFX pipeline run directly in the KFP UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you use utilize the following tools and services to deploy and run a TFX pipeline on Google Cloud that automates the development and deployment of a TensorFlow 2.3 WideDeep Classifer to predict forest cover from cartographic data:\n",
    "\n",
    "* The [**TFX CLI**](https://www.tensorflow.org/tfx/guide/cli) utility to build and deploy a TFX pipeline.\n",
    "* A hosted [**AI Platform Pipeline instance (Kubeflow Pipelines)**](https://www.tensorflow.org/tfx/guide/kubeflow) for TFX pipeline orchestration.\n",
    "* [**Dataflow**](https://cloud.google.com/dataflow) jobs for scalable, distributed data processing for TFX components.\n",
    "* A [**AI Platform Training**](https://cloud.google.com/ai-platform/) job for model training and flock management for parallel tuning trials. \n",
    "* [**AI Platform Prediction**](https://cloud.google.com/ai-platform/) as a model server destination for blessed pipeline model versions.\n",
    "* [**CloudTuner**](https://www.tensorflow.org/tfx/guide/tuner#tuning_on_google_cloud_platform_gcp) and [**AI Platform Vizier**](https://cloud.google.com/ai-platform/optimizer/docs/overview) for advanced model hyperparameter tuning using the Vizier algorithm.\n",
    "\n",
    "You will then create and monitor pipeline runs using the TFX CLI as well as the KFP UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update lab environment PATH to include TFX CLI and skaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Set `PATH` to include the directory containing TFX CLI and skaffold.\n",
    "PATH=%env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate lab package version installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.25.0\n",
      "KFP version: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\"\n",
    "!python -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this lab was built and tested with the following package versions:\n",
    "\n",
    "`TFX version: 0.25.0`  \n",
    "`KFP version: 1.0.4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) If running the above command results in different package versions or you receive an import error, upgrade to the correct versions by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tfx==0.25.0 in /home/jupyter/.local/lib/python3.7/site-packages (0.25.0)\n",
      "Requirement already satisfied: pyyaml<6,>=3.12 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (5.4.1)\n",
      "Requirement already satisfied: attrs<21,>=19.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (20.3.0)\n",
      "Requirement already satisfied: ml-metadata<0.26,>=0.25 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (0.25.1)\n",
      "Requirement already satisfied: tensorflow-transform<0.26,>=0.25 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (0.25.0)\n",
      "Requirement already satisfied: six<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.26,>=0.25 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (0.25.0)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (2.3.4)\n",
      "Requirement already satisfied: tfx-bsl<0.26,>=0.25 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (0.25.0)\n",
      "Requirement already satisfied: absl-py<0.11,>=0.9 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (0.10.0)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.26,>=0.25 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (0.25.0)\n",
      "Requirement already satisfied: docker<5,>=4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (4.4.4)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (1.12.11)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.25 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (2.28.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (1.59.0)\n",
      "Requirement already satisfied: kubernetes<12,>=10.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (11.0.0)\n",
      "Requirement already satisfied: pyarrow<0.18,>=0.17 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (0.17.1)\n",
      "Requirement already satisfied: keras-tuner<2,>=1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (1.0.1)\n",
      "Requirement already satisfied: click<8,>=7 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (7.1.2)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (2.3.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx==0.25.0) (3.20.3)\n",
      "Requirement already satisfied: jinja2<3,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (2.11.3)\n",
      "Requirement already satisfied: tensorflow-cloud<0.2,>=0.1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (0.1.13)\n",
      "Requirement already satisfied: tensorflow-hub<0.10,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tfx==0.25.0) (0.9.0)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (4.1.3)\n",
      "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.17.4)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.0.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (3.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.26.0)\n",
      "Requirement already satisfied: numpy<1.20.0,>=1.14.3 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.18.5)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.4.7)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.8.2)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.4.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.7)\n",
      "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (3.7.4.3)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.18.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.6.0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2021.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.3.1.1)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.9.2.1)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.0.2)\n",
      "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.18.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.7.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.7.3)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.19.3)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.3.2)\n",
      "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.28.3)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.16.3)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.7.3)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.2.2)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.5.31)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.15.5)\n",
      "Requirement already satisfied: google-cloud-build<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.0.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx==0.25.0) (1.2.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx==0.25.0) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx==0.25.0) (1.34.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx==0.25.0) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<3,>=2.7.3->tfx==0.25.0) (1.1.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1->tfx==0.25.0) (1.7.3)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1->tfx==0.25.0) (0.8.9)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1->tfx==0.25.0) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1->tfx==0.25.0) (4.62.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1->tfx==0.25.0) (0.4.4)\n",
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1->tfx==0.25.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.25.0) (2021.10.8)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.25.0) (59.4.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.25.0) (1.26.7)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.25.0) (1.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (3.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (0.37.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (1.13.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-datasets<3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-cloud<0.2,>=0.1->tfx==0.25.0) (3.0.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (from tensorflow-cloud<0.2,>=0.1->tfx==0.25.0) (1.43.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-data-validation<0.26,>=0.25->tfx==0.25.0) (1.3.5)\n",
      "Requirement already satisfied: joblib<0.15,>=0.12 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-data-validation<0.26,>=0.25->tfx==0.25.0) (0.14.1)\n",
      "Requirement already satisfied: tensorflow-metadata<0.26,>=0.25 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-data-validation<0.26,>=0.25->tfx==0.25.0) (0.25.0)\n",
      "Requirement already satisfied: ipython<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (7.30.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (7.6.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx==0.25.0) (1.61.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.16.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.2.7)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.3.3)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.12.3)\n",
      "Requirement already satisfied: proto-plus>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-build<3,>=2.0.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.19.8)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-build<3,>=2.0.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.3.23)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.6.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (3.0.22)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (5.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (4.8.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (3.5.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (1.0.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (6.5.1)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (5.8.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (3.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15.2->tfx==0.25.0) (1.8.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets<3.1.0->tensorflow-cloud<0.2,>=0.1->tfx==0.25.0) (2.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx==0.25.0) (3.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->keras-tuner<2,>=1->tfx==0.25.0) (3.0.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx==0.25.0) (1.42.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.1.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (1.5.1)\n",
      "Requirement already satisfied: importlib-metadata<5 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (4.8.2)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (6.1)\n",
      "Requirement already satisfied: argcomplete>=1.12.3 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (1.12.3)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (7.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.8.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-build<3,>=2.0.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (4.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (6.4.6)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (3.6.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (1.5.4)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (22.3.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (21.1.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (6.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.12.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-build<3,>=2.0.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (0.4.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.25->tfx==0.25.0) (2.21)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.1.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.5.9)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.8.4)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.26,>=0.25->tfx==0.25.0) (21.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: kfp==1.0.4 in /home/jupyter/.local/lib/python3.7/site-packages (1.0.4)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.4) (1.43.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=0.2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (1.8.5)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.4) (4.2.1)\n",
      "Requirement already satisfied: click in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (7.1.2)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (1.35.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.4) (0.8.9)\n",
      "Requirement already satisfied: PyYAML in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (5.4.1)\n",
      "Requirement already satisfied: strip-hints in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (0.1.10)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.4) (2.0.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (11.0.0)\n",
      "Requirement already satisfied: Deprecated in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (1.2.14)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.4) (0.10.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.4) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.4) (0.2.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.4) (1.16.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.4) (4.2.4)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.4) (59.4.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.4) (1.7.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.4) (2.26.0)\n",
      "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.4) (1.34.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.4) (1.3.3)\n",
      "Requirement already satisfied: protobuf in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.4) (3.20.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.4) (4.8.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.4) (20.3.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.4) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.4) (0.18.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.4) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.4) (2.8.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.4) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.4) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.4) (1.2.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==1.0.4) (1.13.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==1.0.4) (0.37.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage>=1.13.0->kfp==1.0.4) (1.61.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage>=1.13.0->kfp==1.0.4) (1.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kfp==1.0.4) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==1.0.4) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==1.0.4) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==1.0.4) (3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jupyter/.local/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==1.0.4) (3.7.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp==1.0.4) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage>=1.13.0->kfp==1.0.4) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage>=1.13.0->kfp==1.0.4) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user tfx==0.25.0\n",
    "%pip install --upgrade --user kfp==1.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to pick up the correct package versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate creation of AI Platform Pipelines cluster\n",
    "\n",
    "Navigate to [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "Note you may have already deployed an AI Pipelines instance during the Setup for the lab series. If so, you can proceed using that instance. If not:\n",
    "\n",
    "**1.  Create or select an existing Kubernetes cluster (GKE) and deploy AI Platform**. Make sure to select `\"Allow access to the following Cloud APIs https://www.googleapis.com/auth/cloud-platform\"` to allow for programmatic access to your pipeline by the Kubeflow SDK for the rest of the lab. Also, provide an `App instance name` such as \"tfx\" or \"mlops\". \n",
    "\n",
    "Validate the deployment of your AI Platform Pipelines instance in the console before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: example TFX pipeline design pattern for Google Cloud\n",
    "The pipeline source code can be found in the `pipeline` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mlops-on-gcp/workshops/tfx-caip-tf23/lab-02-tfx-pipeline/labs/pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\n",
      "drwxr-xr-x 3 jupyter jupyter  4096 Oct 21 04:30 .\n",
      "drwxr-xr-x 4 jupyter jupyter  4096 Oct 21 04:36 ..\n",
      "-rw-r--r-- 1 jupyter jupyter    97 Oct 21 04:30 Dockerfile\n",
      "-rw-r--r-- 1 jupyter jupyter  1666 Oct 21 04:30 config.py\n",
      "-rw-r--r-- 1 jupyter jupyter  1222 Oct 21 04:30 features.py\n",
      "-rw-r--r-- 1 jupyter jupyter 11493 Oct 21 04:30 model.py\n",
      "-rw-r--r-- 1 jupyter jupyter 11084 Oct 21 04:30 pipeline.py\n",
      "-rw-r--r-- 1 jupyter jupyter  2032 Oct 21 04:30 preprocessing.py\n",
      "-rw-r--r-- 1 jupyter jupyter  3282 Oct 21 04:30 runner.py\n",
      "drwxr-xr-x 2 jupyter jupyter  4096 Oct 21 04:30 schema\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config.py` module configures the default values for the environment specific settings and the default values for the pipeline runtime parameters. \n",
    "The default values can be overwritten at compile time by providing the updated values in a set of environment variables. You will set custom environment variables later on this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` module contains the TFX DSL defining the workflow implemented by the pipeline.\n",
    "\n",
    "The `preprocessing.py` module implements the data preprocessing logic  the `Transform` component.\n",
    "\n",
    "The `model.py` module implements the training, tuning, and model building logic for the `Trainer` and `Tuner` components.\n",
    "\n",
    "The `runner.py` module configures and executes `KubeflowDagRunner`. At compile time, the `KubeflowDagRunner.run()` method converts the TFX DSL into the pipeline package in the [argo](https://argoproj.github.io/argo/) format for execution on your hosted AI Platform Pipelines instance.\n",
    "\n",
    "The `features.py` module contains feature definitions common across `preprocessing.py` and `model.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: build your pipeline with the TFX CLI\n",
    "\n",
    "You will use TFX CLI to compile and deploy the pipeline. As explained in the previous section, the environment specific settings can be provided through a set of environment variables and embedded into the pipeline package at compile time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your environment resource settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `GCP_REGION` - the compute region for AI Platform Training, Vizier, and Prediction.\n",
    "- `ARTIFACT_STORE` - An existing GCS bucket. You can use any bucket or use the GCS bucket created during installation of AI Platform Pipelines. The default bucket name will contain the `kubeflowpipelines-` prefix. When specifying the bucket, do not use the trailing slash (/) at the end of the bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-01-c14bd4064600-kubeflowpipelines-default/\n"
     ]
    }
   ],
   "source": [
    "# Use the following command to identify the GCS bucket for metadata and pipeline storage.\n",
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `CUSTOM_SERVICE_ACCOUNT` - In the gcp console Click on the Navigation Menu. Navigate to `IAM & Admin`, then to `Service Accounts` and use the service account starting with prefix - `'tfx-tuner-caip-service-account'`. This enables CloudTuner and the Google Cloud AI Platform extensions Tuner component to work together and allows for distributed and parallel tuning backed by AI Platform Vizier's hyperparameter search algorithm. Please see the lab setup `README` for setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. The endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console. Open the *SETTINGS* for your instance and use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window. The format is `'...pipelines.googleusercontent.com'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set your environment resource settings here for GCP_REGION, ARTIFACT_STORE_URI, ENDPOINT, and CUSTOM_SERVICE_ACCOUNT.\n",
    "GCP_REGION = 'us-east1'\n",
    "ARTIFACT_STORE_URI = 'gs://qwiklabs-gcp-01-c14bd4064600-kubeflowpipelines-default/'\n",
    "ENDPOINT = '2204a389947429c4-dot-us-east1.pipelines.googleusercontent.com'\n",
    "CUSTOM_SERVICE_ACCOUNT = 'tfx-tuner-caip-service-account@qwiklabs-gcp-01-c14bd4064600.iam.gserviceaccount.com'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCP_REGION=us-east1\n",
      "env: ARTIFACT_STORE_URI=gs://qwiklabs-gcp-01-c14bd4064600-kubeflowpipelines-default/\n",
      "env: CUSTOM_SERVICE_ACCOUNT=tfx-tuner-caip-service-account@qwiklabs-gcp-01-c14bd4064600.iam.gserviceaccount.com\n",
      "env: PROJECT_ID=qwiklabs-gcp-01-c14bd4064600\n"
     ]
    }
   ],
   "source": [
    "# Set your resource settings as environment variables. These override the default values in pipeline/config.py.\n",
    "%env GCP_REGION={GCP_REGION}\n",
    "%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "%env CUSTOM_SERVICE_ACCOUNT={CUSTOM_SERVICE_ACCOUNT}\n",
    "%env PROJECT_ID={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the compile time settings to first create a pipeline version without hyperparameter tuning\n",
    "\n",
    "Default pipeline runtime environment values are configured in the pipeline folder `config.py`. You will set their values directly below:\n",
    "\n",
    "* `PIPELINE_NAME` - the pipeline's globally unique name. For each pipeline update, each pipeline version uploaded to KFP will be reflected on the `Pipelines` tab in the `Pipeline name > Version name` dropdown in the format `PIPELINE_NAME_datetime.now()`.\n",
    "\n",
    "* `MODEL_NAME` - the pipeline's unique model output name for AI Platform Prediction. For multiple pipeline runs, each pushed blessed model will create a new version with the format `'v{}'.format(int(time.time()))`.\n",
    "\n",
    "* `DATA_ROOT_URI` - the URI for the raw lab dataset `gs://cloud-training/OCBL203/workshop-datasets`.\n",
    "\n",
    "* `CUSTOM_TFX_IMAGE` - the image name of your pipeline container build by skaffold and published by `Cloud Build` to `Cloud Container Registry` in the format `'gcr.io/{}/{}'.format(PROJECT_ID, PIPELINE_NAME)`.\n",
    "\n",
    "* `RUNTIME_VERSION` - the TensorFlow runtime version. This lab was built and tested using TensorFlow `2.3`.\n",
    "\n",
    "* `PYTHON_VERSION` - the Python runtime version. This lab was built and tested using Python `3.7`.\n",
    "\n",
    "* `USE_KFP_SA` - The pipeline can run using a security context of the GKE default node pool's service account or the service account defined in the `user-gcp-sa` secret of the Kubernetes namespace hosting Kubeflow Pipelines. If you want to use the `user-gcp-sa` service account you change the value of `USE_KFP_SA` to `True`. Note that the default AI Platform Pipelines configuration does not define the `user-gcp-sa` secret.\n",
    "\n",
    "* `ENABLE_TUNING` - boolean value indicating whether to add the `Tuner` component to the pipeline or use hyperparameter defaults. See the `model.py` and `pipeline.py` files for details on how this changes the pipeline topology across pipeline versions. You will create pipeline versions without and with tuning enabled in the subsequent lab exercises for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'tfx_covertype_continuous_training'\n",
    "MODEL_NAME = 'tfx_covertype_classifier'\n",
    "DATA_ROOT_URI = 'gs://cloud-training/OCBL203/workshop-datasets'\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/{}/{}'.format(PROJECT_ID, PIPELINE_NAME)\n",
    "RUNTIME_VERSION = '2.3'\n",
    "PYTHON_VERSION = '3.7'\n",
    "USE_KFP_SA=False\n",
    "ENABLE_TUNING=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_NAME=tfx_covertype_continuous_training\n",
      "env: MODEL_NAME=tfx_covertype_classifier\n",
      "env: DATA_ROOT_URI=gs://cloud-training/OCBL203/workshop-datasets\n",
      "env: KUBEFLOW_TFX_IMAGE=gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training\n",
      "env: RUNTIME_VERSION=2.3\n",
      "env: PYTHON_VERIONS=3.7\n",
      "env: USE_KFP_SA=False\n",
      "env: ENABLE_TUNING=False\n"
     ]
    }
   ],
   "source": [
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env MODEL_NAME={MODEL_NAME}\n",
    "%env DATA_ROOT_URI={DATA_ROOT_URI}\n",
    "%env KUBEFLOW_TFX_IMAGE={CUSTOM_TFX_IMAGE}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERIONS={PYTHON_VERSION}\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env ENABLE_TUNING={ENABLE_TUNING}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your pipeline code\n",
    "\n",
    "You can build and upload the pipeline to the AI Platform Pipelines instance in one step, using the `tfx pipeline create` command. The `tfx pipeline create` goes through the following steps:\n",
    "- (Optional) Builds the custom image to that provides a runtime environment for TFX components or uses the latest image of the installed TFX version \n",
    "- Compiles the pipeline code into a pipeline package \n",
    "- Uploads the pipeline package via the `ENDPOINT` to the hosted AI Platform instance.\n",
    "\n",
    "As you debug the pipeline DSL, you may prefer to first use the `tfx pipeline compile` command, which only executes the compilation step. After the DSL compiles successfully you can use `tfx pipeline create` to go through all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/mlops-on-gcp/workshops/tfx-caip-tf23/lab-02-tfx-pipeline/labs/pipeline/tfx_covertype_continuous_training.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should see a `{PIPELINE_NAME}.tar.gz` file appear in your current pipeline directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: deploy your pipeline container to AI Platform Pipelines with TFX CLI\n",
    "\n",
    "After the pipeline code compiles without any errors you can use the `tfx pipeline create` command to perform the full build and deploy the pipeline. You will deploy your compiled pipeline container hosted on Google Container Registry e.g. `gcr.io/[PROJECT_ID]/tfx_covertype_continuous_training` to run on AI Platform Pipelines with the TFX CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training -> gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training]...\n",
      "[Skaffold] #0 building with \"default\" instance using docker driver\n",
      "[Skaffold] \n",
      "[Skaffold] #1 [internal] load .dockerignore\n",
      "[Skaffold] #1 transferring context: 2B done\n",
      "[Skaffold] #1 DONE 0.3s\n",
      "[Skaffold] \n",
      "[Skaffold] #2 [internal] load build definition from Dockerfile\n",
      "[Skaffold] #2 transferring dockerfile: 134B 0.0s done\n",
      "[Skaffold] #2 DONE 0.2s\n",
      "[Skaffold] \n",
      "[Skaffold] #3 [internal] load metadata for docker.io/tensorflow/tfx:0.25.0\n",
      "[Skaffold] #3 DONE 0.6s\n",
      "[Skaffold] \n",
      "[Skaffold] #4 [internal] load build context\n",
      "[Skaffold] #4 transferring context: 48.30kB 0.0s done\n",
      "[Skaffold] #4 DONE 0.0s\n",
      "[Skaffold] \n",
      "[Skaffold] #5 [1/3] FROM docker.io/tensorflow/tfx:0.25.0@sha256:0700c27c6492b8b2998e7d543ca13088db8d40ef26bd5c6eec58245ff8cdec35\n",
      "[Skaffold] #5 resolve docker.io/tensorflow/tfx:0.25.0@sha256:0700c27c6492b8b2998e7d543ca13088db8d40ef26bd5c6eec58245ff8cdec35 done\n",
      "[Skaffold] #5 sha256:0700c27c6492b8b2998e7d543ca13088db8d40ef26bd5c6eec58245ff8cdec35 2.85kB / 2.85kB done\n",
      "[Skaffold] #5 sha256:05d9b228cf63233a5dc37f474c29ddaadff8c2d0aa7df269802b1641e8eaea51 7.36kB / 7.36kB done\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 0B / 44.53MB\n",
      "[Skaffold] #5 sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84 0B / 10.20MB\n",
      "[Skaffold] #5 sha256:3c2cba919283a210665e480bcbf943eaaf4ed87a83f02e81bb286b8bdead0e75 0B / 49B\n",
      "[Skaffold] #5 sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84 888.05kB / 10.20MB 0.1s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 7.23MB / 44.53MB 0.2s\n",
      "[Skaffold] #5 sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84 5.31MB / 10.20MB 0.2s\n",
      "[Skaffold] #5 sha256:3c2cba919283a210665e480bcbf943eaaf4ed87a83f02e81bb286b8bdead0e75 49B / 49B 0.2s done\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 0B / 317.94MB 0.2s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 13.73MB / 44.53MB 0.3s\n",
      "[Skaffold] #5 sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84 10.20MB / 10.20MB 0.3s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 20.00MB / 44.53MB 0.4s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 28.52MB / 44.53MB 0.5s\n",
      "[Skaffold] #5 sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84 10.20MB / 10.20MB 0.5s done\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 18.33MB / 317.94MB 0.5s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 0B / 324.06MB 0.5s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 35.08MB / 44.53MB 0.6s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 44.53MB / 44.53MB 0.8s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 34.67MB / 317.94MB 0.8s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 21.11MB / 324.06MB 0.9s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 54.82MB / 317.94MB 1.1s\n",
      "[Skaffold] #5 sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 44.53MB / 44.53MB 1.2s done\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 48.92MB / 324.06MB 1.3s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 0B / 162.53MB 1.3s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 78.82MB / 317.94MB 1.6s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 10.40MB / 162.53MB 1.7s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 67.13MB / 324.06MB 1.9s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 19.66MB / 162.53MB 1.9s\n",
      "[Skaffold] #5 extracting sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 103.58MB / 317.94MB 2.1s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 27.80MB / 162.53MB 2.1s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 87.16MB / 324.06MB 2.3s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 36.67MB / 162.53MB 2.3s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 125.58MB / 317.94MB 2.6s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 51.28MB / 162.53MB 2.6s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 104.86MB / 324.06MB 2.8s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 63.79MB / 162.53MB 3.1s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 144.73MB / 317.94MB 3.1s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 122.67MB / 324.06MB 3.2s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 73.38MB / 162.53MB 3.2s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 163.79MB / 317.94MB 3.5s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 85.17MB / 162.53MB 3.5s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 140.71MB / 324.06MB 3.6s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 98.76MB / 162.53MB 3.8s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 180.44MB / 317.94MB 3.9s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 158.41MB / 324.06MB 4.1s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 113.54MB / 162.53MB 4.1s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 197.10MB / 317.94MB 4.3s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 123.14MB / 162.53MB 4.3s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 132.23MB / 162.53MB 4.7s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 175.26MB / 324.06MB 4.8s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 213.09MB / 317.94MB 5.1s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 142.49MB / 162.53MB 5.4s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 194.78MB / 324.06MB 5.6s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 151.19MB / 162.53MB 5.6s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 232.18MB / 317.94MB 5.7s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 251.12MB / 317.94MB 6.0s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 162.53MB / 162.53MB 6.0s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 267.90MB / 317.94MB 6.3s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 221.16MB / 324.06MB 6.3s\n",
      "[Skaffold] #5 extracting sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 5.0s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 317.94MB / 317.94MB 8.3s\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 324.06MB / 324.06MB 8.3s\n",
      "[Skaffold] #5 sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 162.53MB / 162.53MB 6.5s done\n",
      "[Skaffold] #5 sha256:9019978541a7e6c7d3721643cd10251c5394315944e76929056f46cbd6f2e8d6 0B / 7.93MB 8.3s\n",
      "[Skaffold] #5 sha256:9019978541a7e6c7d3721643cd10251c5394315944e76929056f46cbd6f2e8d6 7.93MB / 7.93MB 8.5s\n",
      "[Skaffold] #5 sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 317.94MB / 317.94MB 8.9s done\n",
      "[Skaffold] #5 sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 324.06MB / 324.06MB 8.9s done\n",
      "[Skaffold] #5 sha256:9019978541a7e6c7d3721643cd10251c5394315944e76929056f46cbd6f2e8d6 7.93MB / 7.93MB 8.9s done\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 0B / 739.98MB 9.1s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 863.47kB / 246.48MB 9.1s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 9.59MB / 519.69MB 9.1s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 14.92MB / 246.48MB 9.4s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 35.78MB / 246.48MB 9.9s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 45.48MB / 519.69MB 9.9s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 39.69MB / 739.98MB 10.1s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 50.16MB / 246.48MB 10.2s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 66.40MB / 246.48MB 10.5s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 79.02MB / 519.69MB 10.5s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 78.94MB / 246.48MB 10.7s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 78.94MB / 739.98MB 11.0s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 95.59MB / 246.48MB 11.0s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 108.09MB / 519.69MB 11.2s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 117.03MB / 246.48MB 11.5s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 129.46MB / 246.48MB 11.7s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 137.43MB / 519.69MB 11.8s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 120.94MB / 739.98MB 11.9s\n",
      "[Skaffold] #5 extracting sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 10.2s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 142.78MB / 246.48MB 12.0s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 155.35MB / 246.48MB 12.3s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 170.95MB / 519.69MB 12.6s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 174.37MB / 246.48MB 12.8s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 161.48MB / 739.98MB 13.1s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 191.52MB / 246.48MB 13.2s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 203.14MB / 519.69MB 13.2s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 205.58MB / 246.48MB 13.5s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 219.68MB / 246.48MB 13.8s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 230.65MB / 519.69MB 13.8s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 199.77MB / 739.98MB 13.9s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 235.28MB / 246.48MB 14.2s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 257.23MB / 519.69MB 14.3s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 242.19MB / 739.98MB 14.7s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 287.24MB / 519.69MB 14.9s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 315.03MB / 519.69MB 15.3s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 286.10MB / 739.98MB 15.4s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 344.38MB / 519.69MB 15.8s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 323.10MB / 739.98MB 16.0s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 370.39MB / 519.69MB 16.1s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 364.69MB / 739.98MB 16.5s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 401.50MB / 519.69MB 16.5s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 428.75MB / 519.69MB 17.0s\n",
      "[Skaffold] #5 extracting sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 15.3s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 422.19MB / 739.98MB 17.5s\n",
      "[Skaffold] #5 sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 246.48MB / 246.48MB 17.1s done\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 459.09MB / 519.69MB 17.5s\n",
      "[Skaffold] #5 sha256:a0336ba74309c55f765d1646412fb242fd31141c990741fcb6eec643c731607e 23.27kB / 23.27kB 17.5s\n",
      "[Skaffold] #5 sha256:a0336ba74309c55f765d1646412fb242fd31141c990741fcb6eec643c731607e 23.27kB / 23.27kB 17.8s done\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 488.19MB / 519.69MB 18.0s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 466.67MB / 739.98MB 18.2s\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 519.69MB / 519.69MB 18.5s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 511.97MB / 739.98MB 18.7s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 564.10MB / 739.98MB 19.1s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 616.53MB / 739.98MB 19.6s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 657.47MB / 739.98MB 19.9s\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 704.10MB / 739.98MB 20.3s\n",
      "[Skaffold] #5 extracting sha256:bd47987755ba5e5a41a43bb514db39abded9ea182367ea095d27bb248e3fbf7d 18.8s done\n",
      "[Skaffold] #5 sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 519.69MB / 519.69MB 21.6s done\n",
      "[Skaffold] #5 sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 739.98MB / 739.98MB 24.0s done\n",
      "[Skaffold] #5 extracting sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84\n",
      "[Skaffold] #5 extracting sha256:831c222b21d8d78fd475c38622ca0d892d0899fdda283251f5b02bcb0b945d84 1.5s done\n",
      "[Skaffold] #5 extracting sha256:3c2cba919283a210665e480bcbf943eaaf4ed87a83f02e81bb286b8bdead0e75 done\n",
      "[Skaffold] #5 extracting sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a\n",
      "[Skaffold] #5 extracting sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 5.1s\n",
      "[Skaffold] #5 extracting sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 10.2s\n",
      "[Skaffold] #5 extracting sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 15.2s\n",
      "[Skaffold] #5 extracting sha256:e378d88a5f590a1106078d781af78bddc23e6a26bb5e65e94511ec2d65f5536a 18.3s done\n",
      "[Skaffold] #5 extracting sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df\n",
      "[Skaffold] #5 extracting sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 5.1s\n",
      "[Skaffold] #5 extracting sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 10.2s\n",
      "[Skaffold] #5 extracting sha256:df37508d2f5c9a1e8883b204d04d37b1cbd7754a1bc765b8d72c36f7b90d37df 11.7s done\n",
      "[Skaffold] #5 extracting sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640\n",
      "[Skaffold] #5 extracting sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 5.2s\n",
      "[Skaffold] #5 extracting sha256:c28e7cc900d1694e0e795239419bc679665e8abc02ba40c30d7794836d1d7640 8.5s done\n",
      "[Skaffold] #5 extracting sha256:9019978541a7e6c7d3721643cd10251c5394315944e76929056f46cbd6f2e8d6\n",
      "[Skaffold] #5 extracting sha256:9019978541a7e6c7d3721643cd10251c5394315944e76929056f46cbd6f2e8d6 0.9s done\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 5.1s\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 10.2s\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 15.2s\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 20.2s\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 25.3s\n",
      "[Skaffold] #5 extracting sha256:80dc388c898ccf048dd63c09c4e66748f8a7eeafebb8fa1e3e1e0c8007c0c4da 30.2s done\n",
      "[Skaffold] #5 extracting sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a\n",
      "[Skaffold] #5 extracting sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 5.1s\n",
      "[Skaffold] #5 extracting sha256:afebcf787e04cc5eac5c15ece7fa3cd6665d1256b37ebadd947d93793e1a748a 7.9s done\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 5.1s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 10.2s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 15.4s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 20.5s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 25.7s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 30.8s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 36.0s\n",
      "[Skaffold] #5 extracting sha256:b32cc97043126eb1aaa0ca0b39f1713e76a0c3977054a2a4211788f27d0b2707 38.2s done\n",
      "[Skaffold] #5 extracting sha256:a0336ba74309c55f765d1646412fb242fd31141c990741fcb6eec643c731607e done\n",
      "[Skaffold] #5 DONE 148.6s\n",
      "[Skaffold] \n",
      "[Skaffold] #6 [2/3] WORKDIR ./pipeline\n",
      "[Skaffold] #6 DONE 1.9s\n",
      "[Skaffold] \n",
      "[Skaffold] #7 [3/3] COPY ./ ./\n",
      "[Skaffold] #7 DONE 0.2s\n",
      "[Skaffold] \n",
      "[Skaffold] #8 exporting to image\n",
      "[Skaffold] #8 exporting layers\n",
      "[Skaffold] #8 exporting layers 0.1s done\n",
      "[Skaffold] #8 writing image sha256:06de13f72837f041222735b6a6ee3de40aa11dafcfb1a1f9cd3113c35afd2b3b done\n",
      "[Skaffold] #8 naming to gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training:latest 0.0s done\n",
      "[Skaffold] #8 DONE 0.1s\n",
      "[Skaffold] The push refers to repository [gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training]\n",
      "[Skaffold] f223e35ee456: Preparing\n",
      "[Skaffold] ad28b68cdcef: Preparing\n",
      "[Skaffold] 5dadc0a09248: Preparing\n",
      "[Skaffold] 8fb12d3bda49: Preparing\n",
      "[Skaffold] 2471eac28ba8: Preparing\n",
      "[Skaffold] 674ba689ae71: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 674ba689ae71: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 2471eac28ba8: Layer already exists\n",
      "[Skaffold] 8fb12d3bda49: Layer already exists\n",
      "[Skaffold] 5dadc0a09248: Layer already exists\n",
      "[Skaffold] 674ba689ae71: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] f223e35ee456: Pushed\n",
      "[Skaffold] ad28b68cdcef: Pushed\n",
      "[Skaffold] latest: digest: sha256:1c261cfefaa92d0330af07e4431ea7148d64c5f90fd611affaa3795e5c4014a2 size: 3267\n",
      "[Skaffold] Build [gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training] succeeded\n",
      "[Skaffold] \n",
      "[Skaffold] Help improve Skaffold with our 2-minute anonymous survey: run 'skaffold survey'\n",
      "[Skaffold] To help improve the quality of this product, we collect anonymized usage data for details on what is tracked and how we use this data visit <https://skaffold.dev/docs/resources/telemetry/>. This data is handled in accordance with our privacy policy <https://policies.google.com/privacy>\n",
      "[Skaffold] \n",
      "[Skaffold] You may choose to opt out of this collection by running the following command:\n",
      "[Skaffold] \tskaffold config set --global collect-metrics false\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/mlops-on-gcp/workshops/tfx-caip-tf23/lab-02-tfx-pipeline/labs/pipeline/tfx_covertype_continuous_training.tar.gz\n",
      "{'created_at': datetime.datetime(2023, 10, 21, 5, 3, 13, tzinfo=tzlocal()),\n",
      " 'default_version': {'code_source_url': None,\n",
      "                     'created_at': datetime.datetime(2023, 10, 21, 5, 3, 13, tzinfo=tzlocal()),\n",
      "                     'description': None,\n",
      "                     'id': '3773f2de-2da7-4856-83f6-4177c97e40b0',\n",
      "                     'name': 'tfx_covertype_continuous_training',\n",
      "                     'package_url': None,\n",
      "                     'parameters': [{'name': 'pipeline-root',\n",
      "                                     'value': 'gs://qwiklabs-gcp-01-c14bd4064600-kubeflowpipelines-default//tfx_covertype_continuous_training/{{workflow.uid}}'},\n",
      "                                    {'name': 'data-root-uri',\n",
      "                                     'value': 'gs://cloud-training/OCBL203/workshop-datasets'},\n",
      "                                    {'name': 'eval-steps', 'value': '500'},\n",
      "                                    {'name': 'train-steps', 'value': '5000'}],\n",
      "                     'resource_references': [{'key': {'id': '4c9b2a31-00c7-48b9-a367-a6ff9b27a4cd',\n",
      "                                                      'type': 'PIPELINE'},\n",
      "                                              'name': None,\n",
      "                                              'relationship': 'OWNER'}]},\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'id': '4c9b2a31-00c7-48b9-a367-a6ff9b27a4cd',\n",
      " 'name': 'tfx_covertype_continuous_training',\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://qwiklabs-gcp-01-c14bd4064600-kubeflowpipelines-default//tfx_covertype_continuous_training/{{workflow.uid}}'},\n",
      "                {'name': 'data-root-uri',\n",
      "                 'value': 'gs://cloud-training/OCBL203/workshop-datasets'},\n",
      "                {'name': 'eval-steps', 'value': '500'},\n",
      "                {'name': 'train-steps', 'value': '5000'}],\n",
      " 'resource_references': None,\n",
      " 'url': None}\n",
      "Please access the pipeline detail page at http://2204a389947429c4-dot-us-east1.pipelines.googleusercontent.com/#/pipelines/details/4c9b2a31-00c7-48b9-a367-a6ff9b27a4cd\n",
      "Pipeline \"tfx_covertype_continuous_training\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your code here to use the TFX CLI to deploy your pipeline image to AI Platform Pipelines.\n",
    "!tfx pipeline create  \\\n",
    "--pipeline_path=runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build_target_image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: review the [TFX CLI documentation](https://www.tensorflow.org/tfx/guide/cli#create) on the \"pipeline group\" to create your pipeline. You will need to specify the `--pipeline_path` to point at the pipeline DSL and runner defined locally in `runner.py`, `--endpoint`, and `--build_target_image` arguments using the environment variables specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should see a `build.yaml` file in your pipeline folder created by skaffold. The TFX CLI compile triggers a custom container to be built with skaffold using the instructions in the `Dockerfile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to redeploy the pipeline you can first delete the previous version using `tfx pipeline delete` or you can update the pipeline in-place using `tfx pipeline update`.\n",
    "\n",
    "To delete the pipeline:\n",
    "\n",
    "`tfx pipeline delete --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}`\n",
    "\n",
    "To update the pipeline:\n",
    "\n",
    "`tfx pipeline update --pipeline_path runner.py --endpoint {ENDPOINT}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and monitor a pipeline run with the TFX CLI\n",
    "\n",
    "After the pipeline has been deployed, you can trigger and monitor pipeline runs using TFX CLI.\n",
    "\n",
    "*Hint*: review the [TFX CLI documentation](https://www.tensorflow.org/tfx/guide/cli#run_group) on the \"run group\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: tfx_covertype_continuous_training\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: tfx_covertype_continuous_training\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name                     | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===================================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| tfx_covertype_continuous_training | 0bbaea9e-d894-4ebb-900d-394ce8257ee1 | Pending  | 2023-10-21T05:04:39+00:00 | http://2204a389947429c4-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/0bbaea9e-d894-4ebb-900d-394ce8257ee1 |\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code here to trigger a pipeline run with the TFX CLI\n",
    "!tfx run create --pipeline_name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the status of existing pipeline runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Listing all runs of pipeline: tfx_covertype_continuous_training\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name                     | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===================================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| tfx_covertype_continuous_training | 0bbaea9e-d894-4ebb-900d-394ce8257ee1 | Running  | 2023-10-21T05:04:39+00:00 | http://2204a389947429c4-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/0bbaea9e-d894-4ebb-900d-394ce8257ee1 |\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run list --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the status of a given run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Retrieving run status.\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name                     | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===================================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| tfx_covertype_continuous_training | 0bbaea9e-d894-4ebb-900d-394ce8257ee1 | Running  | 2023-10-21T05:04:39+00:00 | http://2204a389947429c4-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/0bbaea9e-d894-4ebb-900d-394ce8257ee1 |\n",
      "+-----------------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "RUN_ID='0bbaea9e-d894-4ebb-900d-394ce8257ee1'\n",
    "\n",
    "!tfx run status --pipeline_name {PIPELINE_NAME} --run_id {RUN_ID} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important \n",
    "\n",
    "A full pipeline run without tuning enabled will take about 40 minutes to complete. You can view the run's progress using the TFX CLI commands above or in the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise: deploy a pipeline version with tuning enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating automatic model hyperparameter tuning into a continuous training TFX pipeline workflow enables faster experimentation, development, and deployment of a top performing model.\n",
    "\n",
    "The previous pipeline version read from hyperparameter default values in the search space defined in `_get_hyperparameters()` in `model.py` and used these values to build a TensorFlow WideDeep Classifier model.\n",
    "\n",
    "Let's now deploy a new pipeline version with the `Tuner` component added to the pipeline that calls out to the AI Platform Vizier service for distributed and parallelized hyperparameter tuning. The `Tuner` component `\"best_hyperparameters\"` artifact will be passed directly to your `Trainer` component to deploy the top performing model. Review `pipeline.py` to see how this environment variable changes the pipeline topology. Also, review the tuning function in `model.py` for configuring `CloudTuner`.\n",
    "\n",
    "Note that you might not want to tune the hyperparameters every time you retrain your model due to the computational cost. Once you have used `Tuner` determine a good set of hyperparameters, you can remove `Tuner` from your pipeline and use model hyperparameters defined in your model code or use a `ImporterNode` to import the `Tuner` `\"best_hyperparameters\"`artifact from a previous `Tuner` run to your model `Trainer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_TUNING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ENABLE_TUNING=True\n"
     ]
    }
   ],
   "source": [
    "%env ENABLE_TUNING={ENABLE_TUNING}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your pipeline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/mlops-on-gcp/workshops/tfx-caip-tf23/lab-02-tfx-pipeline/labs/pipeline/tfx_covertype_continuous_training.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your pipeline container to AI Platform Pipelines with the TFX CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training -> gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training]...\n",
      "[Skaffold] #0 building with \"default\" instance using docker driver\n",
      "[Skaffold] \n",
      "[Skaffold] #1 [internal] load build definition from Dockerfile\n",
      "[Skaffold] #1 transferring dockerfile:\n",
      "[Skaffold] #1 transferring dockerfile: 31B done\n",
      "[Skaffold] #1 DONE 0.2s\n",
      "[Skaffold] \n",
      "[Skaffold] #2 [internal] load .dockerignore\n",
      "[Skaffold] #2 transferring context: 2B done\n",
      "[Skaffold] #2 DONE 0.2s\n",
      "[Skaffold] \n",
      "[Skaffold] #3 [internal] load metadata for docker.io/tensorflow/tfx:0.25.0\n",
      "[Skaffold] #3 DONE 0.3s\n",
      "[Skaffold] \n",
      "[Skaffold] #4 [1/3] FROM docker.io/tensorflow/tfx:0.25.0@sha256:0700c27c6492b8b2998e7d543ca13088db8d40ef26bd5c6eec58245ff8cdec35\n",
      "[Skaffold] #4 DONE 0.0s\n",
      "[Skaffold] \n",
      "[Skaffold] #5 [internal] load build context\n",
      "[Skaffold] #5 transferring context: 5.29kB done\n",
      "[Skaffold] #5 DONE 0.0s\n",
      "[Skaffold] \n",
      "[Skaffold] #6 [2/3] WORKDIR ./pipeline\n",
      "[Skaffold] #6 CACHED\n",
      "[Skaffold] \n",
      "[Skaffold] #7 [3/3] COPY ./ ./\n",
      "[Skaffold] #7 DONE 0.1s\n",
      "[Skaffold] \n",
      "[Skaffold] #8 exporting to image\n",
      "[Skaffold] #8 exporting layers 0.0s done\n",
      "[Skaffold] #8 writing image sha256:e7d7c2eba1101fa8feaf04905d6b03ecd18c656d34c56f5752f88735badf1e42 done\n",
      "[Skaffold] #8 naming to gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training:latest done\n",
      "[Skaffold] #8 DONE 0.1s\n",
      "[Skaffold] The push refers to repository [gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training]\n",
      "[Skaffold] 024eb775874f: Preparing\n",
      "[Skaffold] ad28b68cdcef: Preparing\n",
      "[Skaffold] 5dadc0a09248: Preparing\n",
      "[Skaffold] 8fb12d3bda49: Preparing\n",
      "[Skaffold] 2471eac28ba8: Preparing\n",
      "[Skaffold] 674ba689ae71: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 674ba689ae71: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] ad28b68cdcef: Layer already exists\n",
      "[Skaffold] 5dadc0a09248: Layer already exists\n",
      "[Skaffold] 2471eac28ba8: Layer already exists\n",
      "[Skaffold] 8fb12d3bda49: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 674ba689ae71: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 024eb775874f: Pushed\n",
      "[Skaffold] latest: digest: sha256:8df0755a3579bcde148f5daf6e308395ab96ed7cece5080f21978a66faa83466 size: 3267\n",
      "[Skaffold] Build [gcr.io/qwiklabs-gcp-01-c14bd4064600/tfx_covertype_continuous_training] succeeded\n",
      "[Skaffold] \n",
      "New container image is built. Target image is available in the build spec file.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/mlops-on-gcp/workshops/tfx-caip-tf23/lab-02-tfx-pipeline/labs/pipeline/tfx_covertype_continuous_training.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2023, 10, 21, 5, 8, 1, tzinfo=tzlocal()),\n",
      " 'description': None,\n",
      " 'id': 'a8edc74d-7338-4977-af1a-e1545a49a7b2',\n",
      " 'name': 'tfx_covertype_continuous_training_20231021050801',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://qwiklabs-gcp-01-c14bd4064600-kubeflowpipelines-default//tfx_covertype_continuous_training/{{workflow.uid}}'},\n",
      "                {'name': 'data-root-uri',\n",
      "                 'value': 'gs://cloud-training/OCBL203/workshop-datasets'},\n",
      "                {'name': 'eval-steps', 'value': '500'},\n",
      "                {'name': 'train-steps', 'value': '5000'}],\n",
      " 'resource_references': [{'key': {'id': '4c9b2a31-00c7-48b9-a367-a6ff9b27a4cd',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at http://2204a389947429c4-dot-us-east1.pipelines.googleusercontent.com/#/pipelines/details/4c9b2a31-00c7-48b9-a367-a6ff9b27a4cd\n",
      "Pipeline \"tfx_covertype_continuous_training\" updated successfully.\n"
     ]
    }
   ],
   "source": [
    "#TODO: your code to update your pipeline \n",
    "!tfx pipeline update --pipeline_path runner.py --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger a pipeline run from the Kubeflow Pipelines UI\n",
    "\n",
    "On the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page, click `OPEN PIPELINES DASHBOARD`. A new browser tab will open. Select the `Pipelines` tab to the left where you see the `PIPELINE_NAME` pipeline you deployed previously. You should see 2 pipeline versions. \n",
    "\n",
    "Click on the most recent pipeline version with tuning enabled which will open up a window with a graphical display of your TFX pipeline directed graph. \n",
    "\n",
    "Next, click the `Create a run` button. Verify the `Pipeline name` and `Pipeline version` are pre-populated and optionally provide a `Run name` and `Experiment` to logically group the run metadata under before hitting `Start` to trigger the pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "A full pipeline run with tuning enabled will take about 50 minutes and can be executed in parallel while the previous pipeline run without tuning continues running. \n",
    "\n",
    "Take the time to review the pipeline metadata artifacts created in the GCS artifact repository for each component including data splits, your Tensorflow SavedModel, model evaluation results, etc. as the pipeline executes. In the GCP console, you can also view the Dataflow jobs for pipeline data processing as well as the AI Platform Training jobs for model training and tuning.\n",
    "\n",
    "When your pipelines runs are complete, review your model versions on Cloud AI Platform Prediction and model evaluation metrics. Did your model performance improve with hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you learned how to build and deploy a TFX pipeline with the TFX CLI and then update, build and deploy a new pipeline with automatic hyperparameter tuning. You practiced triggered continuous pipeline runs using the TFX CLI as well as the Kubeflow Pipelines UI.\n",
    "\n",
    "\n",
    "In the next lab, you will construct a Cloud Build CI/CD workflow that further automates the building and deployment of the TensorFlow WideDeep Classifer pipeline code introduced in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-cpu.2-3.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-3:m87"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
